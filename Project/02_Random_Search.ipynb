{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Regressors\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import log_loss, r2_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_fscore_support, precision_recall_curve, auc\n",
    "\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare parallel kernel\n",
    "\n",
    "*Install [here](https://ipyparallel.readthedocs.io/en/latest/), define the number of engines and click '**Start**' in the* **iPython Clusters** *tab.*\n",
    "\n",
    "Import parallel computing libraries and register processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "from ipyparallel.joblib import IPythonParallelBackend\n",
    "from joblib import Parallel, parallel_backend, register_parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Noto, run `ipcontroller --ip=\"*\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(profile='default')\n",
    "print('profile:', c.profile)\n",
    "print(\"IDs:\", c.ids) # Process id numbers\n",
    "bview = c.load_balanced_view()\n",
    "register_parallel_backend('ipyparallel',\n",
    "                          lambda : IPythonParallelBackend(view=bview))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(feature_importance_sorted, n, type_of_search):\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    x = np.arange(n)\n",
    "    y = [feature_importance_sorted[i][1] for i in range(n)]\n",
    "    labels = [feature_importance_sorted[i][0] for i in range(n)]\n",
    "    ax = sns.barplot(y,x,orient=\"h\");\n",
    "    plt.xlabel(\"Importance fraction\", fontsize = 12)\n",
    "    ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n",
    "    plt.yticks(x,labels, fontsize = 15)\n",
    "    plt.title('Most important feature: {}'.format(type_of_search), fontsize = 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_rf(test_size):\n",
    "    \n",
    "    list_of_files = glob.glob('50_by_100/Full*.csv') # * means all if need specific format then *.csv\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    \n",
    "    print(latest_file)\n",
    "\n",
    "    df_raw = pd.read_csv(latest_file, index_col=0)\n",
    "    df_raw.sort_index(inplace = True, ascending = True)\n",
    "    df_raw.sort_index(inplace = True, axis = 1)\n",
    "\n",
    "    y = df_raw.COVID.copy()\n",
    "    X = df_raw.drop(columns = ['COVID']).copy()\n",
    "    \n",
    "    X_types = dict(X.dtypes)\n",
    "    features = list(X.columns)\n",
    "    \n",
    "    train = np.random.rand(len(df_raw))> test_size\n",
    "\n",
    "    X_train = X[train]\n",
    "    X_test = X[~train]\n",
    "\n",
    "    y_train = y[train].tolist()\n",
    "    y_test = y[~train].tolist()\n",
    "\n",
    "    return X, X_train, X_test, y, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "We will compare two methods, which are grid search and random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_train, X_test, y, y_train, y_test = pp_rf(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "After having explored a grid search, we can adopt another approach. Instead of searching for each value, let's give our model more parameters input, but instead let it choose randomly at each iteration one value for each parameter. It will then be evaluated again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(10,5000,50)\n",
    "criterion = ['mse', 'mae']\n",
    "max_depth = range(5,50,10)\n",
    "min_samples_split = range(2,100,2)\n",
    "min_samples_leaf = range(2,100, 2)\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "bootstrap = [True,False]\n",
    "\n",
    "random_parameters = {'n_estimators' : n_estimators,\n",
    "                     'criterion' : criterion,\n",
    "                     'max_depth' : max_depth,\n",
    "                     'min_samples_split' : min_samples_split,\n",
    "                     'min_samples_leaf' : min_samples_leaf,\n",
    "                     'max_features' : max_features,\n",
    "                     'bootstrap' : bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_reg = RandomizedSearchCV(rf_reg,\n",
    "                                param_distributions = random_parameters,\n",
    "                                n_iter = 50,\n",
    "                                verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with parallel_backend('ipyparallel'):\n",
    "    random_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_best_score = random_reg.best_score_\n",
    "random_best_parameters = random_reg.best_params_\n",
    "random_best_n_estimators = random_best_parameters.get('n_estimators')\n",
    "random_best_criterion = random_best_parameters.get('criterion')\n",
    "random_best_max_depth = random_best_parameters.get('max_depth')\n",
    "random_best_min_samples_split = random_best_parameters.get('min_samples_split')\n",
    "random_best_min_samples_leaf = random_best_parameters.get('min_samples_leaf')\n",
    "random_best_max_features = random_best_parameters.get('max_features')\n",
    "random_best_bootstrap = random_best_parameters.get('bootstrap')\n",
    "\n",
    "\n",
    "print('Random search best_score: {:.5}'.format(random_best_score))\n",
    "\n",
    "print('Random best n_estimators: {}'.format(random_best_n_estimators))\n",
    "print('Random best criterion: {}'.format(random_best_criterion))\n",
    "print('Random best max_depth: {}'.format(random_best_max_depth))\n",
    "print('Random best min_samples_split: {}'.format(random_best_min_samples_split))\n",
    "print('Random best min_samples_leaf: {}'.format(random_best_min_samples_leaf))\n",
    "print('Random best max_features: {}'.format(random_best_max_features))\n",
    "print('Random best bootstrap: {}'.format(random_best_bootstrap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_best_parameters = {'n_estimators' : [random_best_n_estimators],\n",
    "                        'criterion' : [random_best_criterion],\n",
    "                        'max_depth' : [random_best_max_depth],\n",
    "                        'min_samples_split' : [random_best_min_samples_split],\n",
    "                        'min_samples_leaf' : [random_best_min_samples_leaf],\n",
    "                        'max_features' : [random_best_max_features],\n",
    "                        'bootstrap' : [random_best_bootstrap]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_reg_best = GridSearchCV(rf_reg,\n",
    "                               param_grid = random_best_parameters,\n",
    "                               verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend('ipyparallel'):\n",
    "    random_reg_best.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_y_pred = random_reg_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae = abs(random_y_pred - y_test)/y_test\n",
    "print(\"Accuracy: {:.2%}\".format(1 - np.mean(mae)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this accuracy, we can take a deeper look into the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_feature_importances = [(list(X.columns)[i], random_reg_best.best_estimator_.feature_importances_[i])\n",
    "                              for i in range(len(list(X.columns)))]\n",
    "random_feature_importances.sort(key=itemgetter(1), reverse = True)\n",
    "plot_importance(random_feature_importances, 10, 'Ransom search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_test,random_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection\n",
    "\n",
    "Let's try to run the model again, but this time selecting only the most impacting features to save us some work and let's compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selected_features = [random_feature_importances[i][0]\n",
    "                            for i in range(15)]\n",
    "random_X_train_sel = X_train[random_selected_features]\n",
    "random_X_test_sel = X_test[random_selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with parallel_backend('ipyparallel'):\n",
    "    random_reg.fit(random_X_train_sel, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_best_score_sel = random_reg.best_score_\n",
    "random_best_parameters_sel = random_reg.best_params_\n",
    "random_best_n_estimators_sel = random_best_parameters_sel.get('n_estimators')\n",
    "random_best_criterion_sel = random_best_parameters_sel.get('criterion')\n",
    "random_best_max_depth_sel = random_best_parameters_sel.get('max_depth')\n",
    "random_best_min_samples_split_sel = random_best_parameters_sel.get('min_samples_split')\n",
    "random_best_min_samples_leaf_sel = random_best_parameters_sel.get('min_samples_leaf')\n",
    "random_best_max_features_sel = random_best_parameters_sel.get('max_features')\n",
    "random_best_bootstrap_sel = random_best_parameters_sel.get('bootstrap')\n",
    "\n",
    "\n",
    "print('Random search best_score: {:.5}'.format(random_best_score_sel))\n",
    "\n",
    "print('Random best n_estimators: {}'.format(random_best_n_estimators_sel))\n",
    "print('Random best criterion: {}'.format(random_best_criterion_sel))\n",
    "print('Random best max_depth: {}'.format(random_best_max_depth_sel))\n",
    "print('Random best min_samples_split: {}'.format(random_best_min_samples_split_sel))\n",
    "print('Random best min_samples_leaf: {}'.format(random_best_min_samples_leaf_sel))\n",
    "print('Random best max_features: {}'.format(random_best_max_features_sel))\n",
    "print('Random best bootstrap: {}'.format(random_best_bootstrap_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_best_parameters_sel = {'n_estimators' : [random_best_n_estimators_sel],\n",
    "                              'criterion' : [random_best_criterion_sel],\n",
    "                              'max_depth' : [random_best_max_depth_sel],\n",
    "                              'min_samples_split' : [random_best_min_samples_split_sel],\n",
    "                              'min_samples_leaf' : [random_best_min_samples_leaf_sel],\n",
    "                              'max_features' : [random_best_max_features_sel],\n",
    "                              'bootstrap' : [random_best_bootstrap_sel]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_reg_best_sel = GridSearchCV(rf_reg,\n",
    "                                   param_grid = random_best_parameters_sel,\n",
    "                                   return_train_score = True,\n",
    "                                   verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend('ipyparallel'):\n",
    "    random_reg_best_sel.fit(random_X_train_sel, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_y_pred_sel = random_reg_best_sel.predict(random_X_test_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = abs(random_y_pred_sel - y_test)/y_test\n",
    "print(\"Accuracy: {:.2%}\".format(1 - np.mean(mae)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_test,random_y_pred_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
