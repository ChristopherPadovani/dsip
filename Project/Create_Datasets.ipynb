{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Dataset creation\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time, ctime, strftime, gmtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import parallel computing libraries and register processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "from ipyparallel.joblib import IPythonParallelBackend\n",
    "from joblib import Parallel, parallel_backend, register_parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Noto, run `ipcontroller --ip=\"*\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(profile='default')\n",
    "print('profile:', c.profile)\n",
    "print(\"IDs:\", c.ids) # Process id numbers\n",
    "bview = c.load_balanced_view()\n",
    "register_parallel_backend('ipyparallel',\n",
    "                          lambda : IPythonParallelBackend(view=bview))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read datasets, inspired from [this study](https://www.kaggle.com/cgueret/covid-19-risk-factor-predictor/output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('Kaggle/predicted_covid19_risk_factors.csv')\n",
    "targets = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_target_list.csv.gz',\n",
    "                          compression='gzip')\n",
    "diseases = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_disease_list.csv.gz',\n",
    "                           compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore predictions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe predictions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['Risk Factors'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link disease EFO code to Covid Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_extended = predictions.merge(diseases,\n",
    "                              how = 'left',\n",
    "                              left_on = 'Risk Factors',\n",
    "                              right_on = 'disease_full_name')[['efo_id', 'disease_full_name', 'Score']]\n",
    "prediction_extended.sort_values(by = 'efo_id', inplace = True, ascending = True)\n",
    "prediction_extended.reset_index(inplace=True, drop = True)\n",
    "prediction_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('There are {} lines (i.e. diseases) in the dataset.'.format(prediction_extended.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def dir_stats(dir_info = True,\n",
    "              directory = \"200_by_100/\",\n",
    "              file_prefix = \"X_raw_disease_200_target_100_\",\n",
    "              file_suffix = \".csv\"):\n",
    "    n_files = len(glob.glob(directory + file_prefix + \"*\" + file_suffix))\n",
    "    \n",
    "    if dir_info:\n",
    "        print('Number of files in directory {} : {}'.format(directory,n_files))\n",
    "        \n",
    "    return n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_csvs(dir_info = True,\n",
    "             directory = \"200_by_100/\",\n",
    "             file_prefix = \"X_raw_disease_200_target_100_\",\n",
    "             file_suffix = \".csv\"):\n",
    "    \n",
    "    n_files = dir_stats(dir_info = dir_info,\n",
    "                        directory = directory,\n",
    "                        file_prefix = file_prefix,\n",
    "                        file_suffix = file_suffix)\n",
    "\n",
    "    if n_files == 0:\n",
    "        done_diseases = []\n",
    "        done_targets = []\n",
    "        df = None\n",
    "    else:\n",
    "        for i in range(n_files):\n",
    "            if i == 0:\n",
    "                df = pd.read_csv(directory + file_prefix + str(i) + file_suffix, index_col=0)\n",
    "            else:\n",
    "                tmp_df = pd.read_csv(directory + file_prefix + str(i) + file_suffix, index_col=0)\n",
    "                df = df.append(tmp_df)\n",
    "        \n",
    "        done_diseases = df.index.tolist()\n",
    "        done_targets = df.columns.tolist()\n",
    "        done_targets.remove('COVID')\n",
    "    \n",
    "    return done_diseases, done_targets, n_files, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lists(list_info = False,\n",
    "                 disease_ids = prediction_extended.efo_id.tolist(),\n",
    "                 target_ids = targets.ensembl_id.tolist(),\n",
    "                 n_diseases = 200,\n",
    "                 n_targets = 100):\n",
    "\n",
    "    disease_list = np.random.choice(list(disease_ids), n_diseases).tolist()\n",
    "    target_list = np.random.choice(list(target_ids), n_targets).tolist()\n",
    "\n",
    "    disease_list.sort()\n",
    "    target_list.sort()\n",
    "    \n",
    "    if list_info:\n",
    "        print('Number of diseases : ', n_diseases)\n",
    "        print('Number of targets :  ', n_targets)\n",
    "\n",
    "    return disease_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lists(done_diseases: list, done_targets: list, n_diseases):\n",
    "    \n",
    "    disease_ids = prediction_extended[~prediction_extended.efo_id.isin(done_diseases)].efo_id.tolist()\n",
    "    target_ids = targets[~targets.ensembl_id.isin(done_targets)].ensembl_id.tolist()\n",
    "    \n",
    "    disease_list, target_list = create_lists(list_info = False,\n",
    "                                             disease_ids = disease_ids,\n",
    "                                             target_ids = target_ids,\n",
    "                                             n_diseases = n_diseases,\n",
    "                                             n_targets = len(done_targets))\n",
    "    \n",
    "    return disease_list, target_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associations may be checked [here](https://www.targetvalidation.org/disease/EFO_0005774/associations) by replacing the name of the disease in the URL.\n",
    "\n",
    "Associations may also be queried with [the API](https://api.opentargets.io/v3/platform/docs/swagger-ui#/public/getAssociationFilter).\n",
    "\n",
    "Here is a sample query : [https://platform-api.opentargets.io/v3/platform/public/association/filter?disease=DOID_0050890&fields=association_score.overall&fields=target.id&fields=disease.id&size=10000](https://platform-api.opentargets.io/v3/platform/public/association/filter?disease=DOID_0050890&fields=association_score.overall&fields=target.id&fields=disease.id&size=10000)\n",
    "\n",
    "[Open Targets Data Download page](https://www.targetvalidation.org/downloads/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentargets import OpenTargetsClient\n",
    "\n",
    "def build_dataset(disease_list: list, target_list: list):\n",
    "    \n",
    "    n_diseases = len(disease_list)\n",
    "    n_targets = len(target_list)\n",
    "\n",
    "    X_raw = np.zeros((n_diseases,\n",
    "                      n_targets + 1))\n",
    "\n",
    "    ot = OpenTargetsClient()\n",
    "    start_time = time()\n",
    "    tmp_time = time()\n",
    "\n",
    "    print('--------------------------------------------------')\n",
    "    print('BUILD Start time : ', ctime())\n",
    "    print('--------------------------------------------------')\n",
    "\n",
    "    with parallel_backend('ipyparallel'):\n",
    "        \n",
    "        for disease_id, disease_n in zip(disease_list, range(n_diseases)):\n",
    "            \n",
    "            for target_id,target_n in zip(target_list, range(n_targets)):           \n",
    "                search = ot.filter_associations(disease = disease_id,\n",
    "                                                target = target_id,\n",
    "                                                fields = ['association_score.overall',\n",
    "                                                          'target.id',\n",
    "                                                          'disease.id'])\n",
    "                for i, r in enumerate(search):\n",
    "                    if len(search) > 0 and r['disease']['id'] == disease_id:\n",
    "                        X_raw[disease_n][target_n] = r['association_score']['overall']\n",
    "            X_raw[disease_n][n_targets] = prediction_extended.Score[disease_n]\n",
    "            if (disease_n + 1) % 5 == 0:\n",
    "                print('--', disease_n + 1, ' of ', n_diseases)\n",
    "                print('Time : {:.2f} seconds.'.format(time() - start_time))\n",
    "                print('Time since last print : {:.2f} seconds.'.format(time() - tmp_time))\n",
    "                tmp_time = time()\n",
    "    \n",
    "    print('--------------------------------------------------')\n",
    "    print('BUILD End time : ', ctime())\n",
    "    print('--------------------------------------------------')\n",
    "    return X_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_dataset(df: pd.DataFrame,\n",
    "                  disease_list: list,\n",
    "                  target_list: list,\n",
    "                  directory = \"200_by_100/\",\n",
    "                  file_prefix = \"X_raw_disease_200_target_100_\",\n",
    "                  file_n = \"0\",\n",
    "                  file_suffix = \".csv\"):\n",
    "    \n",
    "    pd.DataFrame(df,\n",
    "                 index = disease_list,\n",
    "                 columns = target_list + ['COVID']\n",
    "                ).to_csv(directory + file_prefix + str(file_n) + file_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_dataset(dir_info = True,\n",
    "                       directory = \"200_by_100/\",\n",
    "                       file_prefix = \"X_raw_disease_200_target_100_\",\n",
    "                       file_suffix = \".csv\"):\n",
    "    \n",
    "    _, _, _, df = get_csvs(dir_info = dir_info,\n",
    "                           directory = directory,\n",
    "                           file_prefix = file_prefix,\n",
    "                           file_suffix = file_suffix)\n",
    "    \n",
    "    dt_string = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S-\")\n",
    "    df.to_csv(directory\n",
    "              + \"Full-\" + dt_string\n",
    "              + file_prefix\n",
    "              + str(df.shape[0]) + '_by_' + str(df.shape[1])\n",
    "              + file_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "def create_dataset(n_runs = 1,\n",
    "                   dir_info = True,\n",
    "                   directory = \"200_by_100/\",\n",
    "                   file_prefix = \"X_raw_disease_200_target_100_\",\n",
    "                   file_suffix = \".csv\",\n",
    "                   list_info = False,\n",
    "                   disease_ids = prediction_extended.efo_id.tolist(),\n",
    "                   target_ids = targets.ensembl_id.tolist(),\n",
    "                   n_diseases = 100,\n",
    "                   n_targets = 200):\n",
    "    \n",
    "    g_start_time = ctime()\n",
    "    print('PROGRAM START : ', g_start_time)\n",
    "    print('============================================================')\n",
    "    print('============================================================')\n",
    "    g_start_time_time = time()\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        start_time = ctime()\n",
    "        #print('PROGRAM START : ', start_time)\n",
    "        start_time_time = time()\n",
    "\n",
    "        done_diseases, done_targets, n_files, _ = get_csvs(dir_info = dir_info,\n",
    "                                                           directory = directory,\n",
    "                                                           file_prefix = file_prefix,\n",
    "                                                           file_suffix = file_suffix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if n_files == 0:\n",
    "            disease_list, target_list = create_lists(list_info = list_info,\n",
    "                                                     disease_ids = disease_ids,\n",
    "                                                     target_ids = target_ids,\n",
    "                                                     n_diseases = n_diseases,\n",
    "                                                     n_targets = n_targets)\n",
    "        else:\n",
    "            disease_list, _ = update_lists(done_diseases = done_diseases,\n",
    "                                           done_targets = done_targets,\n",
    "                                           n_diseases = n_diseases)\n",
    "            while not len(disease_list) == len(set(disease_list)):\n",
    "                disease_list, _ = update_lists(done_diseases = done_diseases,\n",
    "                                               done_targets = done_targets,\n",
    "                                               n_diseases = n_diseases)\n",
    "                \n",
    "            target_list = done_targets\n",
    "    \n",
    "\n",
    "        init_time = ctime()\n",
    "        init_time_time = time()\n",
    "        init_time_length = time() - start_time_time\n",
    "        #print('INIT DONE : ', init_time)\n",
    "        #print('INIT DONE IN {} seconds'.format(strftime(\"%H:%M:%S\", gmtime(init_time_length))))\n",
    "\n",
    "        X_raw = build_dataset(disease_list, target_list)\n",
    "\n",
    "        print_dataset(df = X_raw,\n",
    "                      disease_list = disease_list,\n",
    "                      target_list = target_list,\n",
    "                      directory = directory,\n",
    "                      file_prefix = file_prefix,\n",
    "                      file_n = n_files,\n",
    "                      file_suffix = file_suffix)\n",
    "\n",
    "        print_full_dataset(dir_info = dir_info,\n",
    "                           directory = directory,\n",
    "                           file_prefix = file_prefix,\n",
    "                           file_suffix = file_suffix)\n",
    "\n",
    "        done_time = ctime()\n",
    "        done_time_length = time() - init_time_time\n",
    "        #print('PROGRAM DONE : ', done_time)\n",
    "        #print('PROGRAM DONE IN {} seconds'.format(strftime(\"%H:%M:%S\", gmtime(done_time_length))))\n",
    "    \n",
    "        print('============================================================')\n",
    "        print('RUN STARTED : ', start_time)\n",
    "        print('INIT DONE :   ', init_time)\n",
    "        print('RUN DONE :    ', done_time)\n",
    "        print('============================================================')\n",
    "        print('Total execution time : {}'.format(strftime(\"%H:%M:%S\", gmtime(done_time_length))))\n",
    "    \n",
    "    g_done_time = ctime()\n",
    "    g_done_time_length = time() - g_start_time_time\n",
    "    print('============================================================')\n",
    "    print('============================================================')\n",
    "    print('PROGRAM DONE : ', g_done_time)\n",
    "    print('PROGRAM DONE IN {} seconds'.format(strftime(\"%H:%M:%S\", gmtime(g_done_time_length))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_runs = 1\n",
    "\n",
    "dir_info = True\n",
    "directory = \"50_by_100/\"\n",
    "file_prefix = \"X_raw_disease_50_target_100_\"\n",
    "file_suffix = \".csv\"\n",
    "\n",
    "list_info = False\n",
    "disease_ids = prediction_extended.efo_id.tolist()\n",
    "target_ids = targets.ensembl_id.tolist()\n",
    "n_diseases = 50\n",
    "n_targets = 100\n",
    "\n",
    "create_dataset(n_runs = n_runs,\n",
    "               dir_info = dir_info,\n",
    "               directory = directory,\n",
    "               file_prefix = file_prefix,\n",
    "               file_suffix = file_suffix,\n",
    "               list_info = list_info,\n",
    "               disease_ids = disease_ids,\n",
    "               target_ids = target_ids,\n",
    "               n_diseases = n_diseases,\n",
    "               n_targets = n_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_diseases, done_targets, n_files, _ = get_csvs(dir_info = dir_info,\n",
    "                                                           directory = directory,\n",
    "                                                           file_prefix = file_prefix,\n",
    "                                                           file_suffix = file_suffix)\n",
    "\n",
    "print('Done diseases : ', len(done_diseases))\n",
    "print('Unique diseases : ', len(np.unique(done_diseases)))\n",
    "\n",
    "import collections\n",
    "print('============================================================')\n",
    "print('Duplicates : ', [item for item, count in collections.Counter(done_diseases).items() if count > 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
